{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a1a4fd",
   "metadata": {},
   "source": [
    "# Introduction: Machine Learning Project Part 3: Advanced Content Moderation with Transformers\n",
    "\n",
    "In this third part of our machine learning project, we delve into the sophisticated realm of Transformer models, specifically focusing on BERT (Bidirectional Encoder Representations from Transformers) for content moderation. This segment of the project is dedicated to leveraging the advanced capabilities of Transformer architectures to enhance our model's ability to discern and categorize toxic comments in digital communication platforms accurately.\n",
    "\n",
    "![BERT Model](bert.png)\n",
    "\n",
    "Our approach in this phase encompasses:\n",
    "\n",
    "1. **Advanced Data Preprocessing**: Optimizing the dataset for Transformer model compatibility, ensuring the data is well-suited for complex model inputs.\n",
    "2. **Transformer Model Integration**: Implementing the BERT model, renowned for its effectiveness in natural language understanding and text classification tasks.\n",
    "3. **Model Training and Fine-tuning**: Detailed focus on customizing the BERT model to our dataset, meticulously tuning the parameters for optimal performance.\n",
    "4. **Comprehensive Model Evaluation**: Employing a robust set of metrics to evaluate the model's performance, ensuring high accuracy and reliability in classifying various forms of toxic content.\n",
    "5. **Insightful Interpretation of Results**: Deep analysis of the model's predictions to understand its decision-making process and the implications of its classifications.\n",
    "\n",
    "This part of the project is pivotal in harnessing the advanced capabilities of Transformer models, setting a new benchmark in automated content moderation technologies.\n",
    "\n",
    "# Experimentation Explanation\n",
    "Our project's decision to leverage BERT for sentiment classification stems from its breakthrough approach to contextual language understanding. Unlike traditional models that treat language linearly, BERT captures bidirectional context, allowing it to discern nuanced language patterns crucial for sentiment analysis. This becomes particularly useful in differentiating between nuanced expressions of sentiment that may be similar lexically but differ in emotional tone or intensity. The bidirectionality enables BERT to interpret complex sentence structures and idiomatic expressions, providing a robust framework for capturing the subtle cues that define sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b346f",
   "metadata": {},
   "source": [
    "# Library Imports and Initial Setup\n",
    "We start by importing essential libraries, laying the foundation for our advanced machine learning tasks. This includes Pandas for data handling, regular expressions (re) for text processing, NLTK for natural language tasks, and sklearn for model training and evaluation. Additionally, we incorporate Matplotlib for visualizations and NumPy for numerical operations. Crucially, we import specific Transformer-related libraries like BertTokenizer and BertModel, central to our model's architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903608ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Details\n",
    "For BERT to process our Jigsaw dataset effectively, we must transform the raw comments into a structured format that BERT can understand. This transformation includes the tokenization of sentences into word tokens understood by BERT, often breaking down complex words.\n",
    "\n",
    "Moreover, BERT requires attention masks to focus on meaningful content and ignore padding, as well as token type IDs to differentiate between multiple sentences or segments within a single input example. This is especially important in sentiment classification, where the relative position of words can alter the sentiment conveyed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "train_file_path = 'data/train.csv'\n",
    "data = pd.read_csv(train_file_path)\n",
    "\n",
    "#data['comment_text'] = preprocess_data(data['comment_text'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153150</th>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>:Jerome, I see you never got around to this…! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153151</th>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153154</th>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153155</th>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153156</th>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63978 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "5       0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "7       000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "11      0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...   \n",
       "13      0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...   \n",
       "14      00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...   \n",
       "...                  ...                                                ...   \n",
       "153150  fff8f64043129fa2  :Jerome, I see you never got around to this…! ...   \n",
       "153151  fff9d70fe0722906  ==Lucky bastard== \\n http://wikimediafoundatio...   \n",
       "153154  fffa8a11c4378854  ==shame on you all!!!== \\n\\n You want to speak...   \n",
       "153155  fffac2a094c8e0e2  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...   \n",
       "153156  fffb5451268fb5ba  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "5           0             0        0       0       0              0  \n",
       "7           0             0        0       0       0              0  \n",
       "11          0             0        0       0       0              0  \n",
       "13          0             0        0       0       0              0  \n",
       "14          0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "153150      0             0        0       0       0              0  \n",
       "153151      0             0        0       0       0              0  \n",
       "153154      0             0        0       0       0              0  \n",
       "153155      1             0        1       0       1              0  \n",
       "153156      0             0        0       0       0              0  \n",
       "\n",
       "[63978 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the test data\n",
    "test_file_path = 'data/test.csv'  # Replace with the actual path to your test.csv file\n",
    "test_label_path = 'data/test_labels.csv'\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "test_labels = pd.read_csv(test_label_path)\n",
    "\n",
    "# Merge/join the dataframes based on the 'id' column\n",
    "merged_data = pd.merge(test_data, test_labels, on='id')\n",
    "\n",
    "# there are alot of comments that are not givien any labels so just filtering those out\n",
    "filtered_test_data = merged_data[merged_data.iloc[:, 2:].sum(axis=1)>=0] \n",
    "\n",
    "\n",
    "y_test = filtered_test_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].to_numpy()\n",
    "\n",
    "filtered_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis/Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous tasks we have seen that data is really imbalanced \n",
    "so we are going to have a look into how much we can reduce the data\n",
    "We are going to pick all the labeled data (any class labeled as 1)\n",
    "And we are going to take equal number of zero class data\n",
    "\n",
    "It will make the data much small and speed up our training because transformers take a lot of time in training\n",
    "\n",
    "Given the known imbalance in our dataset, we conduct a detailed analysis to understand its extent. Our strategy involves selecting an equal number of labeled (toxic) and unlabeled (non-toxic) comments to balance the dataset, a crucial step for unbiased model training. This section also visualizes the sequence length distribution, aiding in setting appropriate sequence lengths for our Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16225, 8), (16225, 8))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_data = data[data[categories].sum(axis=1) !=0]\n",
    "non_classes_data = data[data[categories].sum(axis=1) ==0]\n",
    "\n",
    "classes_data.shape, non_classes_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to skip some of the zero class examples\n",
    "- Because it will make data much more balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the classed and non classed data with equal number of examples for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(  [non_classes_data.sample(len(classes_data)), classes_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for the nominal sequence lenght for our data\n",
    "\n",
    "- x-axis: Number of words\n",
    "- y-axis: frequency\n",
    "\n",
    "As you can see that almost 95% of the sentences are within 200 word, so we fixed the seq_len to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in data[\"comment_text\"]]\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "max_seq_len = max(seq_len)\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows most comments are within 200 words\n",
    "so 200 tokens would be sufficient for our transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data\n",
    "In this part we will split data to training and validation using using a 85-15 train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"comment_text\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58fc3a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27582, 8), (4868, 8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the training data into training and validation sets (85% - 15%)\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42)\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0831ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "x_train = train_data['comment_text'].to_numpy()\n",
    "x_val = val_data['comment_text'].to_numpy()\n",
    "\n",
    "y_train = train_data[categories].to_numpy()\n",
    "y_val = val_data[categories].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Tokenizer Initialization and Data Tokenization\n",
    "Here, we initialize the BERT tokenizer, a pivotal step in preparing our dataset for the Transformer model. We define a custom dataset class to handle the tokenization process, ensuring that each comment text is appropriately converted into token IDs and attention masks, following the format required by the BERT model.\n",
    "\n",
    "As we are going to use BERT for this part, we need BERT Tokenzier to make the input compatible with BERT model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tiktoken\n",
    "import torch\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Data Preparation\n",
    "\n",
    "We initialize our Transformer model and prepare the dataset for training. This involves setting up the BertTokenizer, defining custom Dataset and DataLoader classes for handling our text data, and ensuring that our data is compatible with the BERT model structure.\n",
    "\n",
    "We split our dataset into training and validation sets. This step is crucial for training our model effectively and evaluating its performance on unseen data, ensuring that our model can generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from gpt4all import Embed4All\n",
    "from pytorch_multilabel_balanced_sampler.samplers import RandomClassSampler, ClassCycleSampler, LeastSampledClassSampler\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    # It was needed to handle the attention masks and token ids\n",
    "    def __init__(self, comment_texts, y):\n",
    "        self.comment_texts = comment_texts\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # max len is 200 because as shown above most comments are covered within 200 words\n",
    "        self.max_len =  200\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment_text = str(self.comment_texts[idx])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.y[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_data['comment_text'].to_list(), torch.tensor(y_train, dtype=torch.float))\n",
    "val_dataset = CustomDataset(val_data['comment_text'].to_list(), torch.tensor(y_val, dtype=torch.float))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True) #sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=48, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c691207",
   "metadata": {},
   "source": [
    "# Model Function and Advantages\n",
    "BERT's self-attention mechanism is a game-changer for sentiment classification. It allows each token to be processed in the context of all others, making it possible to understand not just the sentiment of individual words but also how the sentiment of a word can be affected by the rest of the sentence. For example, negations and conditionals, common in sentiment-laden sentences, are effectively captured by BERT. This deep understanding of context enables BERT to outperform models that might rely on bag-of-words approaches like TF-IDF, which cannot capture such intricacies of language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom model for multi-label sequence classification\n",
    "class BertForMultiLabelSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        \n",
    "        return outputs['logits']\n",
    "\n",
    "# Initialize custom model\n",
    "model = BertForMultiLabelSequenceClassification(num_labels=len(categories))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c1400",
   "metadata": {},
   "source": [
    "# Training Methodology\n",
    "The training process for our sentiment classification model is meticulously crafted to balance between learning from the pre-trained BERT model and adapting to the specific nuances of the Jigsaw dataset. A conservative learning rate of 1e-5 is chosen based on empirical evidence suggesting that it allows for effective fine-tuning without causing the catastrophic forgetting of the valuable pre-trained knowledge. The choice of epochs, batch size, and other hyperparameters is also the result of careful consideration, balancing computational resources with the need for a thorough exploration of the parameter space. The Cross-Entropy Loss function is particularly suited for our multi-class sentiment classification task, as it is designed to work with probabilities and can handle the complexity of multiple sentiment classes.\n",
    "\n",
    "\n",
    "# Visualizations\n",
    "Visual representations of the model's training and evaluation process are crucial for understanding its performance. By plotting loss and accuracy curves over each training epoch, we can monitor the model's progress and make any necessary adjustments. Additionally, visualizations such as confusion matrices provide clarity on how the model performs across different sentiment classes, allowing us to pinpoint areas where the model excels or falls short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1150 [00:00<?, ?it/s]/home/junaid/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_18988/1840981107.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(self.y[idx], dtype=torch.float)\n",
      "Epoch 1/5, Loss: 0.1344: 100%|██████████| 1150/1150 [10:43<00:00,  1.79it/s]\n",
      "Epoch 2/5, Loss: 0.1174: 100%|██████████| 1150/1150 [10:44<00:00,  1.79it/s]\n",
      "Epoch 3/5, Loss: 0.1018: 100%|██████████| 1150/1150 [10:43<00:00,  1.79it/s]\n",
      "Epoch 4/5, Loss: 0.0864: 100%|██████████| 1150/1150 [10:47<00:00,  1.78it/s]\n",
      "Epoch 5/5, Loss: 0.0718: 100%|██████████| 1150/1150 [10:32<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer\n",
    "# We are fine tuning so the learning rate is really small\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-05)\n",
    "\n",
    "device = \"cuda\"\n",
    "# Train the model\n",
    "# training for 5 epochs because it takes a lot of time\n",
    "epochs = 5\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    m_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    for i, batch in enumerate(pbar):\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        \n",
    "        \n",
    "        logits = model(ids, mask, token_type_ids)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Usin BCE because of multi-label setting of data\n",
    "        # with logits because we are not applying any sigmoid over it\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        m_loss += loss.item()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{epochs}, Loss: {m_loss / (i+1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587723c",
   "metadata": {},
   "source": [
    "# Model Evaluation and Validation\n",
    "After training, we rigorously evaluate the model's performance on both validation and test sets. This evaluation is comprehensive, involving various metrics to assess the accuracy, precision, recall, and F1 score of our model. This step is vital to ensure the model's effectiveness and reliability in classifying toxic comments in diverse scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junaid/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_18988/1840981107.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(self.y[idx], dtype=torch.float)\n",
      "  4%|▍         | 4/102 [00:01<00:47,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.91      0.92      0.92       115\n",
      " severe_toxic       0.50      0.38      0.43        13\n",
      "      obscene       0.78      0.88      0.83        60\n",
      "       threat       1.00      0.40      0.57         5\n",
      "       insult       0.69      0.80      0.74        55\n",
      "identity_hate       0.30      0.33      0.32         9\n",
      "\n",
      "    micro avg       0.79      0.83      0.81       257\n",
      "    macro avg       0.70      0.62      0.63       257\n",
      " weighted avg       0.79      0.83      0.81       257\n",
      "  samples avg       0.38      0.40      0.38       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/junaid/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/junaid/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# just validating the model on some examples\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets']\n",
    "\n",
    "        logits = model(ids, mask, token_type_ids)\n",
    "        \n",
    "        y_pred.extend(logits.sigmoid().round().detach().cpu().numpy())\n",
    "        y_true.extend(targets.detach().cpu().numpy())\n",
    "        if len(y_true) > 200:\n",
    "           break\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48414509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "test_dataset = CustomDataset(filtered_test_data['comment_text'].to_list(), torch.tensor(y_test, dtype=torch.float))\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=48, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c46c3",
   "metadata": {},
   "source": [
    "# Evaluation Metrics and Class Imbalance\n",
    "We employ a robust suite of evaluation metrics to comprehensively assess our model's performance. Accuracy provides an overall effectiveness rate, but precision, recall, and the F1-score offer more nuanced insights into the model's ability to handle class imbalances. Precision measures the model's ability to correctly identify positive sentiment instances, while recall gauges its ability to capture all relevant instances. The F1-score, as a harmonic mean of precision and recall, gives us a single metric that balances both concerns – particularly valuable in a dataset like Jigsaw's, which contains imbalanced sentiment classes. To counter the class imbalance, we've implemented stratified sampling and adjusted class weights within the loss function, promoting a more balanced learning and avoiding bias towards any particular sentiment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1333 [00:00<?, ?it/s]/home/junaid/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_18988/1840981107.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(self.y[idx], dtype=torch.float)\n",
      "100%|██████████| 1333/1333 [08:25<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.40      0.96      0.56      6090\n",
      " severe_toxic       0.29      0.41      0.34       367\n",
      "      obscene       0.52      0.83      0.63      3691\n",
      "       threat       0.50      0.58      0.54       211\n",
      "       insult       0.58      0.78      0.66      3427\n",
      "identity_hate       0.43      0.73      0.54       712\n",
      "\n",
      "    micro avg       0.46      0.85      0.59     14498\n",
      "    macro avg       0.45      0.71      0.55     14498\n",
      " weighted avg       0.47      0.85      0.60     14498\n",
      "  samples avg       0.08      0.08      0.08     14498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/junaid/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/junaid/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets']\n",
    "\n",
    "        logits = model(ids, mask, token_type_ids)\n",
    "        y_pred.extend(logits.sigmoid().round().detach().cpu().numpy())\n",
    "        y_true.extend(targets.detach().cpu().numpy())\n",
    "        \n",
    "print(classification_report(y_true, y_pred, target_names=categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b9a62",
   "metadata": {},
   "source": [
    "# In-Depth Results Interpretation\n",
    "A thorough analysis of our model's predictions reveals its proficiency in identifying clear-cut sentiments but also uncovers areas where it could be improved. Instances of sarcasm, cultural references, or context-dependent expressions present challenges that our current model does not always handle well. To address these issues, future work could include training on a dataset enriched with such complex expressions or incorporating a component into the model architecture specifically designed to recognize these nuances, such as a sarcasm detection module or a secondary classifier trained on culturally specific data.\n",
    "\n",
    "**Macro-Avg Comparision between all models:**\n",
    "\n",
    "In increasing order of Macro-Avg:\n",
    "\n",
    "Classifier -> Neural Network -> Transformer\n",
    "\n",
    "Hence we can say that the transformer gave us the best results out of all the models implemented!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a27c3",
   "metadata": {},
   "source": [
    "# Suggested Improvements\n",
    "In future iterations of this project, we aim to refine our sentiment classification model further. This could involve expanding the dataset to include a wider array of sentiments and expressions, potentially scraping additional online forums and social media platforms. Another area for improvement is the exploration of different tokenization strategies to see if they yield better model performance. Experimenting with hyperparameter optimization techniques, such as grid search or random search, could help fine-tune the model's settings to find the optimal balance between training time and accuracy. Additionally, exploring other language models that have emerged since BERT could provide further enhancements in sentiment classification accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
